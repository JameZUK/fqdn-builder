version: '3.8'

services:
  domain-crawler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: domain-crawler
    volumes:
      # Mount input files
      - ./input:/app/input:ro
      # Mount output directory
      - ./output:/app/output
      # Mount logs directory
      - ./logs:/app/logs
      # Mount browser data for persistence
      - ./browser_data:/app/.browser_data
      # Optional: Mount custom cookies
      - ./cookies:/app/cookies:ro
    environment:
      # Set default arguments (can be overridden)
      - CRAWLER_ARGS=--url-file /app/input/targets.txt --fqdn-list --concurrency 3 --output /app/output/domains.txt --log-file /app/logs/crawler.log
      # Browser settings
      - DISPLAY=:99
      - PLAYWRIGHT_BROWSERS_PATH=/home/crawler/.cache/ms-playwright
    working_dir: /app
    restart: unless-stopped
    # Override default command if needed
    command: >
      sh -c "
      echo 'Starting Domain Crawler...' &&
      python domain_crawler.py $${CRAWLER_ARGS}
      "
    # For scheduled runs (optional)
    profiles:
      - manual

  # Optional: Scheduled crawler runs using cron
  domain-crawler-cron:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: domain-crawler-cron
    user: "0"  # Run as root for cron to work properly
    volumes:
      - ./input:/app/input:ro
      - ./output:/app/output
      - ./logs:/app/logs
      - ./browser_data:/app/.browser_data
      - ./cookies:/app/cookies:ro
      - ./crontab:/etc/cron.d/crawler-cron:ro
    environment:
      - CRAWLER_ARGS=--url-file /app/input/targets.txt --fqdn-list --concurrency 3 --output /app/output/domains.txt --log-file /app/logs/crawler.log
      - DISPLAY=:99
      - PLAYWRIGHT_BROWSERS_PATH=/home/crawler/.cache/ms-playwright
    working_dir: /app
    restart: unless-stopped
    command: >
      sh -c "
      echo 'Setting up cron environment...' &&
      mkdir -p /var/run /var/log &&
      touch /var/log/cron.log &&
      if [ -f /etc/cron.d/crawler-cron ]; then
        echo 'Installing crontab from /etc/cron.d/crawler-cron' &&
        chmod 0644 /etc/cron.d/crawler-cron &&
        crontab /etc/cron.d/crawler-cron &&
        echo 'Crontab installed successfully';
      else
        echo 'Warning: No crontab file found at /etc/cron.d/crawler-cron';
      fi &&
      mkdir -p /app/output /app/logs /app/.browser_data &&
      chown -R crawler:crawler /app/output /app/logs /app/.browser_data &&
      echo 'Starting cron daemon...' &&
      cron -f
      "
    profiles:
      - scheduled

  # Optional: Web interface for results viewing
  results-viewer:
    image: nginx:alpine
    container_name: domain-results-viewer
    ports:
      - "8080:80"
    volumes:
      - ./output:/usr/share/nginx/html/output:ro
      - ./logs:/usr/share/nginx/html/logs:ro
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped
    profiles:
      - viewer

networks:
  default:
    name: domain-crawler-network
    enable_ipv6: true
    ipam:
      config:
        - subnet: 172.20.0.0/16
        - subnet: "2001:3984:3989::/64"
